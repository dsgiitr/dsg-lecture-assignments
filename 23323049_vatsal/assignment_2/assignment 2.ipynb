{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6d7f127",
   "metadata": {},
   "source": [
    "# 1.\n",
    "## part1\n",
    "The prior distribution of \n",
    "ùúÉ\n",
    "Œ∏ is given by:\n",
    "$Œ∏‚àºN(5,9)$\n",
    "This means the prior mean \n",
    "$\\mu=5$ and $\\sigma =9$\n",
    "\n",
    "The noise model tells us that the received value x given Œ∏ is normally distributed:\n",
    "\n",
    "$x‚à£Œ∏‚àºN(Œ∏,4)$\n",
    "This means the likelihood mean is Œ∏ and the likelihood variance is $\\sigma^2=4$\n",
    "\n",
    "If the prior distribution of \n",
    "Œ∏ is:\n",
    "\n",
    "$\\theta $~$ N(\\mu_0,{\\sigma_0}^2)$\n",
    "\n",
    "The probability density function (pdf) of the prior is:\n",
    "\n",
    "$p(\\theta)$=$ \\frac{1}{\\sqrt{2\\pi\\sigma_0^2}} exp $(-$\\frac{(\\theta - \\mu_0)^2}{2 \\sigma_0^2}$) \n",
    "\n",
    "$x‚à£Œ∏$~$ N(\\theta,{\\sigma_x}^2)$\n",
    "\n",
    "$p(x|\\theta)$=$ \\frac{1}{\\sqrt{2\\pi\\sigma_x^2}} exp $(-$\\frac{(x-\\theta )^2}{2 \\sigma_x^2}$) \n",
    "\n",
    "$p(\\theta|x) \\propto p(x|\\theta)p(\\theta) \\propto exp(-\\frac{(x-\\theta )^2}{2 \\sigma_x^2} -\\frac{(\\theta - \\mu_0)^2}{2 \\sigma_0^2})$ \n",
    "\n",
    "$-\\frac{(x-\\theta )^2}{2 \\sigma_x^2} -\\frac{(\\theta - \\mu_0)^2}{2 \\sigma_0^2}=$\n",
    "$-\\frac{1}{2} \\left( \\theta^2 \\left( \\frac{1}{\\sigma_x^2} + \\frac{1}{\\sigma_0^2} \\right) - 2 \\theta \\left( \\frac{x}{\\sigma_x^2} + \\frac{\\mu_0}{\\sigma_0^2} \\right) + \\text{const} \\right)$\n",
    "\n",
    "$=-\\frac{1}{2} \\left( \\left( \\theta^2 \\left( \\frac{1}{\\sigma_x^2} + \\frac{1}{\\sigma_0^2} \\right) - 2\\theta \\left( \\frac{x}{\\sigma_x^2} + \\frac{\\mu_0}{\\sigma_0^2} \\right) + \\left( \\frac{x}{\\sigma_x^2} + \\frac{\\mu_0}{\\sigma_0^2} \\right)^2 \\left( \\frac{1}{\\sigma_x^2} + \\frac{1}{\\sigma_0^2} \\right)^{-1} \\right) - \\left( \\frac{x}{\\sigma_x^2} + \\frac{\\mu_0}{\\sigma_0^2} \\right)^2 \\left( \\frac{1}{\\sigma_x^2} + \\frac{1}{\\sigma_0^2} \\right)^{-1} + \\text{constant terms} \\right)$\n",
    "\n",
    "$p(\\theta \\mid x) \\propto \\exp \\left( -\\frac{1}{2} \\left( \\frac{\\left( \\theta - \\left( \\frac{x}{\\sigma_x^2} + \\frac{\\mu_0}{\\sigma_0^2} \\right) \\left( \\frac{1}{\\sigma_x^2} + \\frac{1}{\\sigma_0^2} \\right)^{-1} \\right)^2}{\\left( \\frac{1}{\\sigma_x^2} + \\frac{1}{\\sigma_0^2} \\right)^{-1}} \\right) \\right)\n",
    "$\n",
    "\n",
    "$\\sigma_n^2 = \\left( \\frac{1}{9} + \\frac{1}{4} \\right)^{-1} = \\left( \\frac{4 + 9}{36} \\right)^{-1} = \\left( \\frac{36}{13} \\right) \n",
    "$\n",
    "\n",
    "$\\mu_n = \\frac{\\frac{1}{9} \\cdot 5 + \\frac{1}{4} \\cdot 6}{\\frac{1}{9} + \\frac{1}{4}} = \\frac{74}{13}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f304853",
   "metadata": {},
   "source": [
    "## part 2\n",
    "$$\\mu_{post} =\n",
    "\\frac{\\frac{1}{\\sigma_{prior}^2}\\mu_{prior}+\\frac{n}{\\sigma_{likelihood}^2}\\bar x}\n",
    "{\\frac{1}{\\sigma_{prior}^2}+\\frac{n}{\\sigma_{likelihood}^2}}$$\n",
    "\n",
    "$$œÉ_{post}^2 =\\frac{1}{\\sigma_{prior}^2}+\\frac{n}{\\sigma_{likelihood}^2}$$\n",
    "\n",
    " $\\theta|\\bar x$~$N(\\mu_{post},œÉ_{post}^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba67a3df",
   "metadata": {},
   "source": [
    "## part 3\n",
    "The posterior variance \n",
    "$\\sigma_{post}^2$ decreases, meaning our estimate of $Œ∏$ becomes more precise.\n",
    "\n",
    "The posterior mean $\\mu_{post}$will move closer to the sample mean $\\bar x$ as $n$ increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac39d71",
   "metadata": {},
   "source": [
    "## part 4 \n",
    "### subpart 1\n",
    "The prior distribution of IQ in the general population:\n",
    "$IQ‚àºN(100,152)$\n",
    "This means the prior mean \n",
    " $= 100$ and the prior variance $= 152$\n",
    " \n",
    "The likelihood of the test score given the true IQ:\n",
    "$X‚à£IQ‚àºN(IQ,100)$\n",
    "This means the test score mean is \n",
    "IQ and the test score variance $100$\n",
    "$$E[x]=\\mu_{post}=\\frac{a\\mu_{0}+bX}{a+b}\n",
    "$$\n",
    "$$=\\frac{\\frac{100}{152}+\\frac{80}{100}}{\\frac{1}{152}+\\frac{1}{100}}=87.9$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c44192c",
   "metadata": {},
   "source": [
    "### subpart 2\n",
    "$$E[x]=\\mu_{post}=\\frac{a\\mu_{0}+bX}{a+b}\n",
    "$$\n",
    "$$=\\frac{\\frac{100}{152}+\\frac{150}{100}}{\\frac{1}{152}+\\frac{1}{100}}=130.16$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324764ca",
   "metadata": {},
   "source": [
    "# 2.\n",
    "The pdf of a normal distribution is:\n",
    "$f(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(x - \\mu)^2}{2\\sigma^2} \\right)$\n",
    "\n",
    "For a dataset $x_{1},x_{2},...,x_{n}$, the likelihood function is:\n",
    "$L(\\mu, \\sigma^2) = \\prod_{i=1}^{n} f(x_i \\mid \\mu, \\sigma^2)$\n",
    "\n",
    "Log-Likelihood Function:\n",
    "$\\log L(\\mu, \\sigma^2) = \\sum_{i=1}^{n} \\log f(x_i \\mid \\mu, \\sigma^2) = \\sum_{i=1}^{n} \\left[ -\\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{(x_i - \\mu)^2}{2\\sigma^2} \\right] = -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2$\n",
    "\n",
    "$\\frac{\\partial \\log L(\\mu, \\sigma^2)}{\\partial \\mu} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu) = 0 \\implies \\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} x_i$\n",
    "\n",
    "$\\frac{\\partial \\log L(\\mu, \\sigma^2)}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\sum_{i=1}^{n} (x_i - \\mu)^2 = 0 \\implies \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\hat{\\mu})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9b99962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE estimate of mean : 4.909486585019609\n",
      "MLE estimate of standard deviation : 1.9740663173380517\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)  \n",
    "true_mu = 5\n",
    "true_sigma = 2\n",
    "data = np.random.normal(true_mu, true_sigma, 1000)\n",
    "\n",
    "def mle_gaussian(data):\n",
    "    n = len(data)\n",
    "    mu_mle = np.mean(data)\n",
    "    sigma_mle = np.sqrt(np.sum((data - mu_mle) ** 2) / n)\n",
    "    return mu_mle, sigma_mle\n",
    "\n",
    "# Calculate MLE estimates\n",
    "mu_mle, sigma_mle = mle_gaussian(data)\n",
    "\n",
    "print(f\"MLE estimate of mean : {mu_mle}\")\n",
    "print(f\"MLE estimate of standard deviation : {sigma_mle}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36c1b1d",
   "metadata": {},
   "source": [
    "# 3.\n",
    "$P(y \\mid X, w) = \\prod_{i=1}^{N} \\sigma(x_i^T w)^{y_i} (1 - \\sigma(x_i^T w))^{1 - y_i}$\n",
    "\n",
    "let the prior be $P(w) = \\mathcal{N}(w \\mid 0, \\tau^2 I) \\propto \\exp \\left( -\\frac{1}{2\\tau^2} w^T w \\right)\n",
    "$ \n",
    "\n",
    "$P(w \\mid X, y) \\propto P(y \\mid X, w) P(w)$\n",
    "\n",
    "$\\log P(w \\mid X, y) = \\sum_{i=1}^{N} \\left[ y_i \\log \\sigma(x_i^T w) + (1 - y_i) \\log (1 - \\sigma(x_i^T w)) \\right] - \\frac{1}{2\\tau^2} w^T w\n",
    "$\n",
    "\n",
    "The MAP estimate of \n",
    "ùë§ is the value that maximizes the log-posterior.\n",
    "$w_{map}=argmax(\\sum_{i=1}^{N} \\left[ y_i \\log \\sigma(x_i^T w) + (1 - y_i) \\log (1 - \\sigma(x_i^T w)) \\right] - \\frac{1}{2\\tau^2} w^T w)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a01f881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import expit\n",
    "\n",
    "def log_posterior(w, X, y, tau):\n",
    "   \n",
    "    # Log-likelihood part\n",
    "    z = X.dot(w)\n",
    "    log_likelihood = y.dot(np.log(expit(z))) + (1 - y).dot(np.log(1 - expit(z)))\n",
    "    #log prior part\n",
    "    log_prior = -0.5 * np.sum(w**2) / tau**2\n",
    "    return -(log_likelihood + log_prior)\n",
    "\n",
    "def map_estimate(X, y, tau):\n",
    "    \n",
    "    initial_w = np.zeros(X.shape[1])\n",
    "    \n",
    "    # Minimize the negative log-posterior\n",
    "    result = minimize(log_posterior, initial_w, args=(X, y, tau), method='L-BFGS-B')\n",
    "    \n",
    "    return result.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba2f9df",
   "metadata": {},
   "source": [
    "# 4. VC dimension\n",
    "constant function = 1\n",
    "\n",
    "Linear Function in d Dimensions = d+1\n",
    "\n",
    "Axis-Aligned Rectangle in 2-Dimensions = 4\n",
    "\n",
    "Intervals = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d1d0e9",
   "metadata": {},
   "source": [
    "# 5. KL divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d49d328",
   "metadata": {},
   "source": [
    "## part 1\n",
    "$D_{kl}(p(x)||q(x))=\\int_{-\\infty}^{\\infty} p(x)ln\\frac{p(x)}{q(x)}dx$\n",
    "\n",
    "$p(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_1} \\exp\\left( -\\frac{(x - \\mu_1)^2}{2\\sigma_1^2} \\right)$\n",
    "\n",
    "$q(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_2} \\exp\\left( -\\frac{(x - \\mu_2)^2}{2\\sigma_2^2} \\right)$\n",
    "\n",
    "$D_{KL}(p \\parallel q) = \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi}\\sigma_1} \\exp\\left( -\\frac{(x-\\mu_1)^2}{2\\sigma_1^2} \\right) \\log \\left( \\frac{\\sqrt{2\\pi}\\sigma_2}{\\sqrt{2\\pi}\\sigma_1} \\exp\\left(\\frac{(-\\frac{(x-\\mu_1)^2}{2\\sigma_1^2})}{(-\\frac{(x-\\mu_2)^2}{2\\sigma_2^2})}  \\right) \\right) dx\n",
    "$\n",
    "\n",
    "simplifying\n",
    "$\\log \\left( \\frac{\\sqrt{2\\pi}\\sigma_2}{\\sqrt{2\\pi}\\sigma_1} \\exp\\left(\\frac{(-\\frac{(x-\\mu_1)^2}{2\\sigma_1^2})}{(-\\frac{(x-\\mu_2)^2}{2\\sigma_2^2})}  \\right) \\right) dx = \\log\\left(\\frac{\\sigma_1}{\\sigma_2}\\right) + \\frac{2\\sigma_2^2}{\\sigma_1^2} (x - \\mu_2)^2 - \\frac{2\\sigma_1^2}{\\sigma_2^2} (x - \\mu_1)^2\n",
    "$\n",
    "\n",
    "$\\begin{aligned}\n",
    "& \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi}\\sigma_1} \\exp\\left( -\\frac{(x-\\mu_1)^2}{2\\sigma_1^2} \\right) \\log\\left(\\frac{\\sigma_1}{\\sigma_2}\\right) dx \\\\\n",
    "& = \\log\\left(\\frac{\\sigma_1}{\\sigma_2}\\right) \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi}\\sigma_1} \\exp\\left( -\\frac{(x-\\mu_1)^2}{2\\sigma_1^2} \\right) dx \\\\\n",
    "& = \\log\\left(\\frac{\\sigma_1}{\\sigma_2}\\right) \\\\\n",
    "& \\int_{-\\infty}^{\\infty} \\frac{1}{2\\pi\\sigma_1}\\exp\\left( -\\frac{(x-\\mu_1)^2}{2\\sigma_1^2} \\right) \\left(\\frac{(x-\\mu_2)^2}{2\\sigma_2^2}\\right) dx \\\\\n",
    "& = \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} \\\\\n",
    "& \\int_{-\\infty}^{\\infty} \\frac{1}{2\\pi\\sigma_1} \\exp\\left( -\\frac{(x-\\mu_1)^2}{2\\sigma_1^2} \\right) \\left(\\frac{(x-\\mu_1)^2}{2\\sigma_1^2}\\right) dx \\\\\n",
    "& = \\frac{1}{2}\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "$D_{KL}(N(\\mu_1, \\sigma_1^2) \\parallel N(\\mu_2, \\sigma_2^2)) = \\log\\left(\\frac{\\sigma_1}{\\sigma_2}\\right) + \\frac{2\\sigma_2^2}{\\sigma_1^2} + \\left(\\frac{\\mu_1 - \\mu_2}{\\sigma_2}\\right)^2 - \\frac{1}{2}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75fa21f",
   "metadata": {},
   "source": [
    "## part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8af9679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_csv('data_KL.csv')\n",
    "\n",
    "df['ln(p/q)'] = np.log(df['P'] / df['Q'])\n",
    "kl = np.sum(df['P']*df['ln(p/q)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e47885fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>P</th>\n",
       "      <th>Q</th>\n",
       "      <th>ln(p/q)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7.888609e-31</td>\n",
       "      <td>1.486720e-07</td>\n",
       "      <td>-53.593195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7.888609e-29</td>\n",
       "      <td>2.438962e-07</td>\n",
       "      <td>-49.483025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3.904861e-27</td>\n",
       "      <td>3.961301e-07</td>\n",
       "      <td>-46.066052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.275588e-25</td>\n",
       "      <td>6.369829e-07</td>\n",
       "      <td>-43.054697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3.093301e-24</td>\n",
       "      <td>1.014086e-06</td>\n",
       "      <td>-40.331280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             P             Q    ln(p/q)\n",
       "0           0  7.888609e-31  1.486720e-07 -53.593195\n",
       "1           1  7.888609e-29  2.438962e-07 -49.483025\n",
       "2           2  3.904861e-27  3.961301e-07 -46.066052\n",
       "3           3  1.275588e-25  6.369829e-07 -43.054697\n",
       "4           4  3.093301e-24  1.014086e-06 -40.331280"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4158996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31815510041414485"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c90c6de",
   "metadata": {},
   "source": [
    "## part 3\n",
    "KL Divergence represrnt the divergence not the distance between the two distributions and also it is not symmetric.\n",
    "one way of finding the distance could be taking the absolute difference between p($x_{i}$) and q($x_{i}$) and then taking the maximum and minimum of them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25bd905",
   "metadata": {},
   "source": [
    "# 6. mixture of gaussians\n",
    "## part 1\n",
    "we can use the EM algorithm for mixture of gaussians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7e4d152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of D1: [5.93675083e+03 2.49829361e+00]\n",
      "Mean of D2: [2072.63804569    2.52201437]\n",
      "Variance of D1: [1.59305645e+06 1.63133898e+01]\n",
      "Variance of D2: [1.6079402e+06 1.5969658e+01]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "mixed_data = pd.read_csv('data_open.csv')\n",
    "\n",
    "num_components = 2\n",
    "\n",
    "gmm = GaussianMixture(n_components=num_components)\n",
    "\n",
    "gmm.fit(mixed_data)\n",
    "\n",
    "means = gmm.means_\n",
    "covariances = gmm.covariances_\n",
    "\n",
    "mean_D1 = means[0]\n",
    "mean_D2 = means[1]\n",
    "variance_D1 = np.diag(covariances[0])\n",
    "variance_D2 = np.diag(covariances[1])\n",
    "\n",
    "print(\"Mean of D1:\", mean_D1)\n",
    "print(\"Mean of D2:\", mean_D2)\n",
    "print(\"Variance of D1:\", variance_D1)\n",
    "print(\"Variance of D2:\", variance_D2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b389d356",
   "metadata": {},
   "source": [
    "## part 2\n",
    "Fit GMM: Use the EM algorithm to fit a GMM to the mixed dataset. \n",
    "\n",
    "Extract Parameters: After fitting the GMM, you can obtain the means, variances, and mixing coefficients of the Gaussian components.\n",
    "\n",
    "Identify Components: Identify which Gaussian components correspond to D1 and D2 by comparing their means and variances.\n",
    "\n",
    "Estimate Ratio: We can use the mixing coefficients of these components to estimate the ratio of data points taken from each distribution. The mixing coefficient represents the proportion of data points assigned to each component by the GMM.\n",
    "\n",
    "Ratio Calculation: The ratio of data points taken from D1 and D2 can be estimated as the ratio of the mixing coefficients of the corresponding Gaussian components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c7dafa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
