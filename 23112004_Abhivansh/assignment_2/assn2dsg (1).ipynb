{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9S4YHQ904zV"
      },
      "source": [
        "$Q-1$ Bayesian Probability\\\n",
        "Given:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& \\theta \\sim N(5, 9) \\\\\n",
        "& \\mu_1=5,  \\sigma_1{}^2=9 \\\\\n",
        "& x / \\theta \\sim N(\\theta, 4) \\\\\n",
        "& \\mu_2=\\theta, \\sigma_2{ }^2=4 \\\\\n",
        "&\n",
        "\\end{aligned}\n",
        "$$\n",
        "$1.1 Part 1 :-$\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& x=6 \\\\\n",
        "& P\\left(\\frac{x}{\\theta}\\right)=\\frac{1}{\\sqrt{2 \\pi} \\sigma_2} e^{-\\left(\\frac{(x-\\theta)^2}{2 \\sigma_2^2}\\right)} \\\\\n",
        "& P\\left(\\frac{6}{\\theta}\\right)=\\frac{1}{\\sqrt{2 \\pi}*\\sqrt{4}} e^{-\\left(\\frac{(6-\\theta)^2}{2*4}\\right)}\n",
        "&=\\frac{1}{\\sqrt{8 \\pi}} e^{-\\left(\\frac{(6-\\theta)^2}{8}\\right)}\n",
        "\\end{aligned}\n",
        "$$\n",
        "Prior:\n",
        "$$\n",
        "p(\\theta)=\\frac{1}{\\sqrt{2 \\pi}*3} e^{-\\left(\\frac{(\\theta-5)^2}{18}\\right)}\n",
        "$$\n",
        "\n",
        "According to Bayes' Theorem, posterior :-\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& P(\\theta / x)=\\frac{P(x / \\theta) \\cdot P(\\theta)}{P(x)} \\\\\n",
        "& P(\\theta / x)\\propto\\frac{1}{12 \\pi} e^{-\\left(\\frac{13}{72} \\theta^2{}-\\frac{148}{72} \\theta+\\frac{424}{72}\\right)} \\\\\n",
        "&\n",
        "\\end{aligned}\n",
        "$$\n",
        "Solving the exponential further ;\n",
        "$$\n",
        "\\begin{gathered}\n",
        "-\\frac{13}{72}\\left(\\theta^2-\\frac{148}{13} \\theta+\\frac{424}{13}\\right) \\\\\n",
        "=\\frac{-13}{72}\\left(\\left(\\theta-\\frac{74}{13}\\right)^2-\\left(\\frac{74}{13}\\right)^2+\\frac{424}{13}\\right)\n",
        "=-\\frac{13}{72}\\left(\\theta-\\frac{74}{13}\\right)^2+\\text { constant } \\\\\n",
        "So;\n",
        "\n",
        "\\mu=\\frac{74}{13} \\\\\n",
        "\\sigma^2=\\frac{72}{2 \\times 13}=\\frac{36}{13}\n",
        "\\end{gathered}\n",
        "$$\n",
        "\n",
        "Thus, the posterior distribution for $\\theta$ given $x=6$ is.\n",
        "$$\n",
        "\\frac{\\theta}{x=6} \\sim N\\left(\\frac{74}{13}, \\frac{36}{13}\\right)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltkPpsgB04zb"
      },
      "source": [
        "$1.2-$ Part 2 :\n",
        "The known values are:\n",
        "- The noise in each signal is normally distributed with variance $\\sigma^2=4$.\n",
        "- The prior distribution of $\\theta$ is $N(5,9)$.\\\n",
        "So $\\mu_{prior}= 5$ and $\\sigma_{prior}^2= 9$\\\n",
        "Given $\\sigma_{likelihood}^2= 4$\n",
        "###$a= \\frac{1}{\\sigma_{prior}^2} = \\frac{1}{9}, b=\\frac{n}{\\sigma_{likelihood}^2}=\\frac{n}{4}$\n",
        "Substituting these values, we get:\\\n",
        "###$\\mu_{posterior} = \\frac {20+ 9n \\bar{x}}{9n+4}$\n",
        "###$\\sigma_{posterior}^2= \\frac{36}{4+9n}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6WKYtRD04zc"
      },
      "source": [
        "$1.3-$ Part 3 :\n",
        "\n",
        "The interpretation of the update equations:\n",
        "\n",
        " - Posterior Variance : As we can observe, as the value of $n$ increases i.e. as the number of samples increases, posterior variance decreases. Initially,the posterior variance is large reflecting the uncertainty of $\\theta$. As n tends to $\\infty$, post. variance limits to 0, depicting it's convergence and giving high confidence on the value of $\\theta$.\n",
        " - Posterior Mean : Observing the posterior mean term, as $n$ increases, the weight on the prior mean (5) decreases and the weight on the sample mean $\\bar{x}$ increases. When $n$ is small, the prior mean has a stronger influence on the posterior mean, but when more data is collected, post. mean moves closer to $\\bar{x}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ9S_THf04zc"
      },
      "source": [
        "$1.4-$ Part 4 :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guBX1QUR04zd"
      },
      "source": [
        "In this problem, we are going to use the formulas mentioned above (Normal-Normal Bayesian Updating). Let's define the parameters' distribution:\n",
        "- Sub Part 1\\\n",
        "Prior Distribution: \\\n",
        "$\\theta \\sim N(100, 152)$ \\\n",
        "$ \\frac{x}{\\theta} \\sim N(0,102)$ \\\n",
        "So $\\mu_{prior} =100 , \\sigma_{prior}^2 =152 , \\sigma_{likelihood}^2 = 102$\\\n",
        "Acc. to the equations:\\\n",
        "$a = \\frac{1}{\\sigma_{prior}^2} , b= \\frac{1}{\\sigma_{likelihood}^2}$\n",
        "## $\\mu_{posterior}= \\frac{a\\mu_{prior} + bx}{a+b} , \\sigma_{posterior}^2= \\frac{1}{a+b}$\n",
        "So calculating it further, given $x=80$,\n",
        "$a= 0.0065, b= 0.0098$ \\\n",
        "$\\mu_{posterior}$ comes out to be $87.9754$ and $\\sigma_{posterior}^2= 61.34$\\\n",
        "Expected Value of his true IQ = $88$\n",
        "-Sub Part 2\\\n",
        "Given $x=150$, \\\n",
        "$\\mu_{posterior} = 130.0613 , \\sigma_{posterior}^2$ remains same as $61.34$\\\n",
        "Expected value of his true IQ = $130$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxaMo3-u04zd"
      },
      "source": [
        "$Q-2$ - Maximum Likelihood Estimation\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& \\text { Given } : N(\\mu, \\sigma^2) \\\\\n",
        "& f(x)=\\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\n",
        "\\end{aligned}\n",
        "$$\n",
        "Suppose, we have $n$ samples ;\n",
        "$$\n",
        "x_1, x_2, x_3 \\ldots ,x_n\n",
        "$$\n",
        "\n",
        "Likelihood function :\n",
        "$L(\\theta)\\Rightarrow f\\left(x_1\\right) \\cdot f\\left(x_2\\right) \\cdot f\\left(x_3\\right) \\cdots f\\left(x_n\\right) =\\frac{1}{(2 \\pi)^{n/2} (\\sigma)^n} e^{-\\frac{1}{2 \\sigma^2}\\left[\\left(x_1-\\mu\\right)^2+\\left(x_2-\\mu\\right)^2+\\cdots\\right.]}$ \\\\\n",
        "\n",
        "$\\log (L(\\theta))=-\\frac{n}{2} \\log 2 \\pi-n \\log \\sigma -\\frac{1}{2 \\sigma^2}\\left(\\left(x_1-\\mu\\right)^2+\\left(x_2-\\mu\\right)^2\\right. \\left.+\\cdots\\left(x_n-\\mu\\right)^2\\right)$ \\\\\n",
        "$=-\\frac{n}{2} \\log 2 \\pi-n \\log \\sigma-\\frac{1}{2 \\sigma^2}\\left (x_1^2+x_1^2+\\ldots x_n{ }^2\\right)+n \\mu^2\\right. -2 \\mu\\left(\\sum_{i=1}^n x_i\\right)$\n",
        "\n",
        "\n",
        "\\begin{aligned}\n",
        "& \\frac{d(\\log (L(\\theta))}{d \\mu}=0 \\\\\n",
        "& \\not 2 n \\mu-\\not 2 \\sum_{i=1}^n x_i=0 \\\\\n",
        "& \\left\\lvert\\, \\mu=\\frac{\\sum_{i=1}^n x_i}{n}\\right.\n",
        "\\end{aligned}\n",
        "\n",
        "\n",
        "\\begin{aligned}\n",
        "& \\frac{d(\\log(L(\\theta))}{d \\sigma} =0\\\\\n",
        "& 0=-\\frac{n}{\\sigma}+\\frac{1}{\\sigma^3}\\left[\\sum_{i=1}^n\\left(x_i\\right)^2+n \\mu^2-2 \\mu \\sum_{i=1}^n\\left(x_i\\right)\\right] \\\\\n",
        "& n \\sigma^2=\\sum_{i=1}^n\\left(x_i\\right)^2+n \\mu^2-2 \\mu \\sum_{i=1}^n x_i \\\\\n",
        "& \\sigma=\\frac{1}{n} \\sum_{i=1}^n\\left(x_i\\right)^2+\\mu^2-2 \\mu \\sum_{i=1}^n \\frac{x_i}{n} \\\\\n",
        "& \\sigma^2=\\frac{\\sum\\left(x_i-\\mu\\right)^2}{n} \\\\\n",
        "&\n",
        "\\end{aligned}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6fi2Y2D04ze",
        "outputId": "856dd90e-1144-42c5-beb4-812c58670cfb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_16812\\1761300287.py:4: DeprecationWarning: `np.math` is a deprecated alias for the standard library `math` module (Deprecated Numpy 1.25). Replace usages of `np.math` with `math`\n",
            "  from numpy import math\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "from scipy.optimize import minimize\n",
        "from numpy import math\n",
        "\n",
        "x= np.random.normal(3,17,1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNn7Rl_T04zi",
        "outputId": "0e03adbd-258f-4506-d6e7-cd097087e696"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estimated Mean: 3.167227412109315\n",
            "Estimated Variance: 16.86697563292754\n"
          ]
        }
      ],
      "source": [
        "def normal_pdf(x,mu,sigma):\n",
        "    pdf = [(math.e)**(-0.5*(x-mu)**2/(sigma)**2)]/(2*(math.pi)*(sigma)**2)**0.5\n",
        "    return pdf\n",
        "\n",
        "def log_likelihood(x, mu, sigma):\n",
        "    n = len(x)\n",
        "    return -n/2 *math.log(2*math.pi)-n*math.log(sigma)-sum((x -mu)**2)/(2*sigma**2)\n",
        "\n",
        "def neg_log_likelihood(params):\n",
        "    mu, sigma = params\n",
        "    return -log_likelihood(x, mu, sigma)\n",
        "\n",
        "ini_guess=3,17\n",
        "min=minimize(neg_log_likelihood,ini_guess)\n",
        "\n",
        "estimated_mu, estimated_sigma= min.x\n",
        "print(\"Estimated Mean:\", estimated_mu)\n",
        "print(\"Estimated Variance:\", estimated_sigma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7AB6RiC04zj"
      },
      "source": [
        "$Q-3$ - Maximum a Posterior Estimation\n",
        "\n",
        "MAP with a uniform prior is equivalent to MLE but this is not the same with Gaussian prior.\n",
        "Logistic Regression hypothesis:\n",
        "\n",
        "$h_{\\theta}(x) = \\frac{1}{1+e^-{\\theta^T x}}$\n",
        "where $\\theta$ is the parameter matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JZHc5-Jx04zk"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def map(beta, x, mu, sigma, y):\n",
        "    z = np.dot(x, beta)\n",
        "    likelihood = np.sum(y*np.log(sigmoid(z)) + (1-y)*np.log(1-sigmoid(z)))\n",
        "    prior = -0.5 * np.sum(((beta-mu)/sigma)**2)-(len(beta)/2) * np.log(2 * math.pi * sigma**2)\n",
        "    return likelihood + prior\n",
        "\n",
        "def neg_map(beta, x, mu, sigma, y):\n",
        "    return -map(beta, x, mu, sigma, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUrVnlak04zl",
        "outputId": "c0e468a8-c97c-4d3d-f321-227c527d9a2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estimated Parameters: [ 0.43997372  0.81692846 -1.49912604]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "N=1000\n",
        "X=np.random.randn(N, 3)\n",
        "t_beta=np.array([0.5, 1, -1.5])\n",
        "y=(np.random.rand(N) < sigmoid(np.dot(X, t_beta))).astype(int)\n",
        "\n",
        "mu_prior=np.zeros(X.shape[1])\n",
        "sigma_prior=16\n",
        "initial_guess=np.zeros(X.shape[1])\n",
        "result=minimize(neg_map, initial_guess, args=(X, mu_prior, sigma_prior, y))\n",
        "estimated_beta=result.x\n",
        "\n",
        "print(f\"Estimated Parameters: {estimated_beta}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvJWYf9h04zm"
      },
      "source": [
        "$Q-4$ - VC Dimension\\\n",
        "$4.1-$ Let H be the set of constant functions, and consider any set of n points in the input space. If n = 0 (no points), then all functions in H can shatter this set, as there are no points to classify.\n",
        "If n = 1 (one point), then the set of constant functions H can shatter this single point by either assigning it to the positive or negative class.\n",
        "However, if n >= 2 (two or more points), then no constant function in H can shatter this set, as it will assign the same label to all points.\n",
        "Therefore, the maximum number of points that can be shattered by the set of constant functions H is 1, which implies that the VC dimension of H is 1.\\\n",
        "$4.2-$ Linear funciton can be represented as $f(x)= w_0 + w_1 x_1+ w_2 x_2 + \\cdots + w_m x_m$\\\n",
        "If we have d + 1 points in general position , then we can find a linear function that separates these points in any desired way (positive or negative category). This is because a hyperplane in d dimensions has d degrees of freedom, and we can adjust the weights (w₀, w₁, ..., wₘ) to position the hyperplane such that it separates the d + 1 points as desired.\n",
        "However, if we have d + 2 or more points, then there exists at least one set of points that cannot be shattered by any linear function. This is because a hyperplane in d dimensions can only separate the points into two regions, but we need to assign d + 2 points to different classes, which is not possible with a single hyperplane.\n",
        "Hence VC dim for this concept class is $d+1$.\n",
        "For example, in 2D space (d = 2), a linear function is a line, and the VC dimension is 3. This means that any set of three points in 2D can be shattered (classified in all possible ways) by a line, but no set of four or more points can be shattered by a line.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KlaDc-D04zm"
      },
      "source": [
        "$Q-5$ - KL Divergence \\\n",
        "$Part$ -1\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& K L(p \\| q)=\\int[(\\log (p(x))-(\\log q(x))] p(x) d x \\\\\n",
        "& p(x)=\\frac{1}{\\sqrt{2 \\pi} \\sigma_1} e^{-\\frac{1}{2}\\left(\\frac{x-\\mu_1}{\\sigma_1}\\right)^2} ; q(x)=\\frac{1}{\\sqrt{2 \\pi} \\sigma_2} e^{-\\frac{1}{2}\\left(\\frac{x-\\mu_2}{\\sigma_2}\\right)^2} \\\\\n",
        "& \\log (p(x))=-\\log \\left(\\sqrt{2 \\pi} \\sigma_1\\right)-\\frac{1}{2}\\left(\\frac{x-\\mu_1}{\\sigma_1}\\right)^2 \\\\\n",
        "& \\log (q(x))=-\\log \\left(\\sqrt{2 \\pi} \\sigma_2\\right)-\\frac{1}{2}\\left(\\frac{x-\\mu_1}{\\sigma_2}\\right)^2 \\\\\n",
        "& \\log (p(x)) - \\log (q(x))=\\log \\left(\\frac{\\sigma_2}{\\sigma_1}\\right)+\\frac{1}{2}\\left(\\frac{x-\\mu_2}{\\sigma_2}\\right)^2-\\frac{1}{2}\\left(\\frac{x-\\mu_1}{\\sigma_1}\\right)^2 \\\\\n",
        "& =\\log \\left(\\frac{\\sigma_2}{\\sigma_1}\\right)+\\frac{1}{2}\\left[\\frac{\\left(\\sigma_1^2-\\sigma_2^2 ) x^2+\\left(\\sigma_1^2 \\mu_2^2-\\sigma_2^2 \\mu_1^2 \\right)+\\cdots\\right.}{\\left(\\sigma_1 \\sigma_2\\right)^2}\\right] \\\\\n",
        "&\n",
        "\\end{aligned}\n",
        "$$\n",
        "Integral.\n",
        "$(I).   \\int_{-\\infty}^{\\infty} \\log \\left(\\frac{\\sigma_2}{\\sigma_1}\\right) \\frac{1}{\\sqrt{2 \\pi} \\sigma^2} e^{-\\frac{1}{2}\\left(\\frac{x-\\omega_1}{\\sigma}\\right)^2} dx=\\frac{1}{\\sqrt{2 \\pi} \\sigma_1} \\log \\left(\\frac{\\sigma}{\\sigma_1}\\right) \\times \\sqrt{2 \\pi} \\sigma_1=\\log \\left(\\frac{\\sigma_2}{\\sigma_1}\\right)$\n",
        "\n",
        "$(II).   \\int_{-\\infty}^{\\infty} \\frac{1}{2}\\left[\\frac{\\left(\\sigma_1^2-\\sigma_2^2\\right) x^2+\\left(\\sigma_1^2 \\mu_2^2-\\sigma_2^2 \\mu_1^2\\right)+\\left(2 \\mu_1 \\sigma_1^2-2 \\mu_2 \\sigma_1^2\\right) x}{\\left(\\sigma_1 \\sigma_2\\right)^2}\\right]. \\frac{1}{\\sqrt{2 \\pi} \\sigma_1} e^{-\\frac{1}{2}\\left(\\frac{x-\\mu_1}{\\sigma_1}\\right)^2} dx $\n",
        "\n",
        "$= \\frac{\\frac {\\sqrt{2 \\pi}\\sigma_1^3 + \\sqrt{2 \\pi}\\sigma_1(\\mu_1 - \\mu_2)^2}{2 \\sigma_2^2} - \\frac {\\sqrt{2 \\pi} \\sigma_1}{2}} {\\sqrt{2 \\pi} \\sigma_1}= \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2 \\sigma_2^2} - \\frac{1}{2}$\n",
        "\n",
        "$\\Rightarrow (I) + (II) \\Rightarrow \\log(\\frac{\\sigma_2}{\\sigma_1})+\\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2 \\sigma_2^2} - \\frac{1}{2} $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAsCnIGX04zm"
      },
      "source": [
        "$Part$ -2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sg00oh8w04zn",
        "outputId": "615815a1-00cc-4013-c6d0-8362f03db08e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBCGqjSV2KCh"
      },
      "outputs": [],
      "source": [
        "dataset= pd.read_csv(\"/content/drive/MyDrive/data_KL.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LKhZ2XH04zn"
      },
      "outputs": [],
      "source": [
        "def KL_divergence_product(p,q):\n",
        "    return (p *(np.log(p/q)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "pb-3F2XKD_Zw",
        "outputId": "5142482c-2790-4860-cc83-bcd47c2baa8f"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "MzPLZxK32TAi",
        "outputId": "0da86483-d909-4f32-e0ad-ec2f87cb1324"
      },
      "outputs": [],
      "source": [
        "df=pd.DataFrame(columns=[\"kl_divergence\"])\n",
        "for i in range(len(dataset)):\n",
        "    kl_divergence=KL_divergence_product(dataset[\"P\"][i],dataset[\"Q\"][i])\n",
        "    df.loc[i]=[kl_divergence]\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cvulvn1SGPNw"
      },
      "outputs": [],
      "source": [
        "final_score=0\n",
        "for i in df[\"kl_divergence\"]:\n",
        "  final_score+=i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSzt5a0SGeyG",
        "outputId": "358713b7-ddc5-417b-8008-526837c1c925"
      },
      "outputs": [],
      "source": [
        "print(final_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The final scores came out to be $0.318$ and $0.82$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "it0ae32c6GJB"
      },
      "source": [
        "$Part$ -3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4CXaiAaCbzr"
      },
      "source": [
        "- KL Divergence Intuition: The most fundamental intuition of the KL divergence score is that, it represents/tells us about the difference in two distributions, basically the statistical distance. In other words, how much information is lost when we approximate one distribution with another.\n",
        "- An approach to find out distance between two distributions can be, taking distances between each entry and hence, the final summation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev-rnvYpLMY3"
      },
      "source": [
        "$Q-6$ - Open Ended Add On"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "IEP-2ZhZJhfu",
        "outputId": "ed50e9b7-b187-48b0-a468-ba175c1ac15e"
      },
      "outputs": [],
      "source": [
        "dataset_open=pd.read_csv(\"/content/drive/MyDrive/data_open.csv\")\n",
        "dataset_open"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "bCZ0xsCHOiIO",
        "outputId": "8479e64e-2b79-4245-e615-0909f4ca2ffe"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.hist(dataset_open['data'], bins=10, histtype='bar', ec='black')\n",
        "plt.title('Data distribution over time')\n",
        "plt.xlabel('Data')\n",
        "plt.ylabel('Frequency')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "ugA8kWXjMc5s",
        "outputId": "2994720f-6e69-4c17-ed29-c97c59c573b7"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(n_clusters=2)\n",
        "kmeans.fit(dataset_open)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28tJv2RIYQ-4",
        "outputId": "e89d9ffe-730f-4e8d-a62c-e738942bb1b0"
      },
      "outputs": [],
      "source": [
        "kmeans.cluster_centers_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVEWblruU548"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def separate_normal_distributions(X):\n",
        "    mean = sum(X) / len(X)\n",
        "    std_dev = math.sqrt(sum((x-mean)**2 for x in X) / len(X))\n",
        "    min_val, max_val = min(X), max(X)\n",
        "    num_bins = 20\n",
        "    bin_width = (max_val - min_val) / num_bins\n",
        "    bin_counts = [0] * num_bins\n",
        "    for x in X:\n",
        "        bin_index = int((x-min_val)//bin_width)\n",
        "        bin_counts[bin_index] += 1\n",
        "\n",
        "    #identifying the two bins with the highest counts (potential peaks)\n",
        "    peak1_index, peak2_index =sorted(range(num_bins), key=lambda i: bin_counts[i], reverse=True)[:2]\n",
        "    peak1=min_val+peak1_index*bin_width\n",
        "    peak2=min_val+peak2_index*bin_width\n",
        "    sep_point=(peak1 + peak2)/2\n",
        "\n",
        "    #splitting in two subsets\n",
        "    X1, X2 = [], []\n",
        "    for x in X:\n",
        "        if x < sep_point:\n",
        "            X1.append(x)\n",
        "        else:\n",
        "            X2.append(x)\n",
        "    convergence_value = 0.01\n",
        "    while True:\n",
        "        mean1 = sum(X1) / len(X1)\n",
        "        std_dev1 = math.sqrt(sum((x - mean1) ** 2 for x in X1) / len(X1))\n",
        "        mean2 = sum(X2) / len(X2)\n",
        "        std_dev2 = math.sqrt(sum((x - mean2) ** 2 for x in X2) / len(X2))\n",
        "\n",
        "        #reassigning the outlier data points\n",
        "        X1_new, X2_new = [], []\n",
        "        for x in X1:\n",
        "            if abs(x - mean1) <= 3 * std_dev1:\n",
        "                X1_new.append(x)\n",
        "            else:\n",
        "                X2_new.append(x)\n",
        "        for x in X2:\n",
        "            if abs(x - mean2) <= 3 * std_dev2:\n",
        "                X2_new.append(x)\n",
        "            else:\n",
        "                X1_new.append(x)\n",
        "        sep_point_new = (sum(X1_new) / len(X1_new) + sum(X2_new) / len(X2_new)) / 2\n",
        "\n",
        "        if abs(sep_point_new - sep_point) < convergence_value:\n",
        "            break\n",
        "        X1, X2 = X1_new, X2_new\n",
        "        sep_point = sep_point_new\n",
        "    return X1, X2\n",
        "separate_normal_distributions(dataset_open['data'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
