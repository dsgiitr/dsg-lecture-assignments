{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOLUTION 1 <br><br>\n",
    "PART 1 <br>\n",
    "\n",
    "x ~ $N(\\theta,4)$<br>\n",
    "$\\theta$ ~ $N(5,9)$<br>\n",
    "\n",
    "bayesian update scheme :- <br>\n",
    "$p(\\theta | x) \\propto L(x| \\theta) * p(\\theta)$\n",
    "\n",
    " L(x|$\\theta$) = $ \\frac{1}{\\sqrt{2\\pi 4 }} \\ e^{-\\frac{(x-\\theta)^2}{2 * 4}} = \\frac{1}{\\sqrt{8\\pi}}e^{- \\frac{(x - \\theta)^2}{8}} $   ....(Likelihood function)\n",
    "\n",
    "$P(\\theta) = \\frac{1}{\\sqrt{2\\pi 9 }} e^{- \\frac{(\\theta - 5)^2}{2*4}} = \\frac{1}{3\\sqrt{2\\pi}}e^{- \\frac{(\\theta - 5)^2}{18}}$\n",
    "\n",
    "$P(\\theta | x ) \\propto e^{- \\frac{(x - \\theta)^2}{8} - \\frac{(\\theta - 5 )^2}{18}}$  (ignoring the constant term)<br>\n",
    "\n",
    "$P(\\theta | x ) \\propto e^{- \\frac{(6 - \\theta)^2}{8} - \\frac{(\\theta - 5 )^2}{18}}$\n",
    "\n",
    "$P(\\theta | x ) \\propto e^{-\\frac{1}{8}(36 - 12\\theta + \\theta^2) + \\frac{1}{18}(\\theta^2 - 10\\theta + 25)}$\n",
    "\n",
    "$P(\\theta | x ) \\propto e^{-\\frac{9}{72}(36 - 12\\theta + \\theta^2) + \\frac{4}{72}(\\theta^2 - 10\\theta + 25)}$\n",
    "\n",
    "$P(\\theta | x ) \\propto e^{-\\frac{324}{72} + \\frac{148}{72}\\theta - \\frac{13}{72} \\theta^2 - \\frac{100}{72}}$\n",
    "\n",
    "$P(\\theta | x ) \\propto e^{-\\frac{13}{72}\\theta^2 + \\frac{148}{72}\\theta - \\frac{424}{72}}$\n",
    "\n",
    "$P(\\theta | x ) \\propto e^{-\\frac{13}{72}(\\theta^2) - \\frac{148}{13}\\theta + (\\frac{148}{13*2})^2 - (\\frac{148}{13*2})^2 + \\frac{424}{13}}$\n",
    "\n",
    "$P(\\theta | x ) \\propto e^{-\\frac{13}{72}[(\\theta - \\frac{74}{13})^2]}$\n",
    "\n",
    "$P(\\theta | x ) \\propto e^{-\\frac{1}{2*\\frac{36}{13}}(\\theta - \\frac{74}{13})^2}$\n",
    "\n",
    "from this we get \n",
    "$\\theta$ ~ $N(\\frac{74}{13} , \\frac{36}{13})$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOLUTION 2 \n",
    "\n",
    "Probability density function for gaussian distribution is  p($\\mu , \\sigma | x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^-\\frac{(x-\\mu)^2}{2 \\sigma^2} $                   ...... (1.1)\n",
    "<br>\n",
    "Likelihood function for datapoints $x_1 , x_2 , ... , x_n $ is : <br>\n",
    "\n",
    "$ L(\\mu , \\sigma | x_1 , x_2 , ... , x_n) = \\prod_{i=1} ^{n} p (\\mu,\\sigma | x_i)$                 ..... (1.2)\n",
    "\n",
    "Finding maximum likelihood function(MLE):<br><br>\n",
    "Taking log on both sides in equation (1.2) to ease the process <br>\n",
    "$ Log[L(\\mu , \\sigma | x_1 , x_2 , ... , x_n)] = \\sum_{i=1}^{n} log[p(\\mu, \\sigma|x_i)] $ <br>\n",
    "\n",
    "substituting value of p($\\mu , \\sigma |x$) from eqn (1.1)\n",
    "\n",
    "Log[L($\\mu , \\sigma | x_1 ,...,x_n )] = - \\frac{n}{2}Log(2\\pi) - nLog(\\sigma) - \\frac{(x_1 - \\mu)^2}{2\\sigma^2} - ... - \\frac{(x_n - \\mu)^2}{2\\sigma^2} $ <br><br>\n",
    "\n",
    "differenting Log[L($\\mu , \\sigma | x_1 ,...,x_n $)] with respect to $\\mu $, keeping $\\sigma$ constant : \n",
    "\n",
    "$ \\frac{\\partial Log[L(\\mu , \\sigma | x_1 ,...,x_n )]}{\\partial \\mu} $ = $\\frac{1}{\\sigma^2}[(x_1,...,x_n) - n\\mu]$\n",
    "\n",
    "$ \\frac{\\partial Log[L(\\mu , \\sigma | x_1 ,...,x_n )]}{\\partial \\sigma} $ = $ -\\frac{n}{\\sigma} + \\frac{(x_1 - \\mu^2)}{\\sigma^3} + ... + \\frac{(x_n - \\mu)^2}{\\sigma^3}$\n",
    "\n",
    "Inorder to maximize liklihood function: <br> \n",
    "$ \\frac{\\partial Log[L(\\mu , \\sigma | x_1 ,...,x_n )]}{\\partial \\mu} $ = 0 and  $ \\frac{\\partial Log[L(\\mu , \\sigma | x_1 ,...,x_n )]}{\\partial \\sigma} $ = 0\n",
    "<br><br>\n",
    "thus we get :<br><br>\n",
    "$$ \\mu = \\frac{x_1 , x_2,...,x_n}{n}$$ \n",
    "$$ \\sigma = \\sqrt{\\frac{(x_1 - \\mu)^2 + ... + (x_n - \\mu)^2}{n}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true mean of the data was -  0 and estimated mean is 0.02654906487108962\n",
      "true variance of the data was -  1 and estimated variance is 1.014919037516215\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "obsn = 1000\n",
    "true_mean = 0\n",
    "true_variance = 1\n",
    "data = np.random.normal(true_mean , true_variance, obsn)\n",
    "\n",
    "mean_data = np.mean(data)\n",
    "var_data = np.var(data)\n",
    "\n",
    "print(\"true mean of the data was - \", true_mean , \"and estimated mean is\" , mean_data)\n",
    "\n",
    "print(\"true variance of the data was - \", true_variance , \"and estimated variance is\" , var_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the above code , we get to know the effectiveness of MLE method to get the unknown variables by seeing how close the values of estimated parameters are to the true parameters (mean and variance) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOLUTION 3 <br><br>\n",
    "\n",
    "Let <br>\n",
    "y = vector of binary labels(0 or 1)<br>\n",
    "x = design matrix with features for each data point<br> \n",
    "w = weight vector of logistic regression model<br>\n",
    "$\\sigma^2$ = variance of gaussian prior <br>\n",
    "<br>\n",
    "$sigmoid(w^T x_i) = \\frac{1}{1+e^{-w^T x_i}}$<br>\n",
    "probability density function :-<br>\n",
    "$P(y_i | x_i , w) = sigmoid(w^T x_i)$  ...if $y_i = 0$  <br>\n",
    "$P(y_i | x_i , w) = 1 - sigmoid(w^T x_i)$  ...if $y_i = 1$\n",
    "<br><br>\n",
    "(A)Likelihood function :-<br>\n",
    "Likelihood function is the product of individual probabilities across all datapoints . <br>\n",
    "Thus $L(y_i | x_i , w) = \\prod_{i = 1}^{n}  (\\frac{1}{1+e^{-w^T x_i}})^{y_i} \\  (1-\\frac{1}{1+e^{-w^T x_i}})^{1-y_i}$<br><br>\n",
    "\n",
    "(B)Prior distribution :- <br>\n",
    "Given that there is gaussian prior on model parameters , thus <br>\n",
    "p(w) = $\\frac{1}{\\sigma\\sqrt{2\\pi}} e^-\\frac{(w-\\mu)^2}{2 \\sigma^2} $  \n",
    "<br><br>\n",
    "\n",
    "(C)Posterior distribution :-<br>\n",
    "Probability of w given data <br>\n",
    "Posterior distribution is proportional to product of likelihood and prior<br>\n",
    "P(w|y,x) = L(y|x,w) x P(w)     ...(1.1)<br><br>\n",
    "\n",
    "Log posterior(taking log on both sides of equation (1.1)) : <br>\n",
    "\n",
    "Log(P(w|y,x)) = $\\sum_{i =1}^{n} [y_i \\ log \\ (sigmoid(w^T x_i)) + (1-y_i) \\ log \\ (1-sigmoid(w^T x_i))]$ - $\\frac{w^T w}{2\\sigma^2}$\n",
    "<br><br>\n",
    "\n",
    "To find MAP estimate:<br>\n",
    "we maximize the log posterior with respect to w <br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOLUTION 5 <br><br>\n",
    "PART 1 <br>\n",
    "KL divergence = $\\int p(x) \\ log \\frac{p(x)}{q(x)} \\ dx$ \n",
    "\n",
    "p(x)= $N(\\mu_1, \\sigma_1 ^2)$ = $\\frac{1}{\\sigma_1\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu_1)^2}{2 \\sigma_1^2}} $ <br>\n",
    "q(x)= $N(\\mu_2, \\sigma_2 ^2)$ = $\\frac{1}{\\sigma_2\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu_2)^2}{2 \\sigma_2^2}} $ <br>\n",
    "\n",
    "KL divergence = $\\int p(x) \\ log [p(x)] \\ dx$  + $\\int p(x) \\ log [q(x)] \\ dx$ .....(1.1)\n",
    "<br>\n",
    "\n",
    "$\\int p(x) \\ log [p(x)] \\ dx$ = $- \\frac{(1 + log \\ 2\\pi\\sigma_1 ^2)}{2}$ ....(1.2)\n",
    "<br><br>\n",
    "$- \\int p(x) \\ log [q(x)] \\ dx$ = $ - \\int p(x)$ log $[\\frac{1}{\\sigma_2\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu_2)^2}{2 \\sigma_2^2}}]$ dx  \n",
    "<br>\n",
    "$\\ \\ \\ \\ $= $\\frac{1}{2}log(2\\pi\\sigma_2 ^2) - \\int p(x) \\ log \\ e^{-\\frac{(x-\\mu)^2}{2\\sigma_2 ^2}} dx$<br>\n",
    "\n",
    "$\\ \\ \\ \\ $= $\\frac{1}{2}log(2\\pi\\sigma_2 ^2) +  \\int p(x) \\ \\frac{(x-\\mu)^2}{2\\sigma^2} dx$<br>\n",
    "$\\ \\ \\ \\ $= $\\frac{1}{2}log(2\\pi\\sigma_2 ^2) + \\frac{1}{2\\sigma_2 ^2}[\\int p(x)x^2 dx + \\int p(x) \\mu_2 ^2 dx - 2\\int p(x)x\\mu_2 dx]$<br>\n",
    "\n",
    "$\\ \\ \\ \\ $= $\\frac{1}{2}log(2\\pi\\sigma_2 ^2) + \\frac{1}{2\\sigma_2 ^2}[<x^2> + \\mu_2 ^2 - 2\\mu_2 <x> dx]$\n",
    "<br>\n",
    "since ,<br> $\\int p(x) dx = 1$<br>\n",
    "$\\int p(x) x dx = <x>$ <br>\n",
    "$\\int p(x) x^2 dx = <x^2> $ <br> \n",
    "where <.> denotes the expectation operator <br>\n",
    "\n",
    "also<br> \n",
    "$<x>=\\mu_1$  and<br>\n",
    "$<x^2> = \\sigma_1 ^2 + \\mu_1 ^2 $\n",
    "<br><br>\n",
    "\n",
    "$\\ \\ \\ \\ $= $\\frac{1}{2}log(2\\pi\\sigma_2 ^2) + \\frac{1}{2\\sigma_2 ^2}[\\mu_1 ^2 + \\sigma_1 ^2 + \\mu_2 ^2- 2\\mu_2\\mu_1]$<br>\n",
    "$- \\int p(x) \\ log [q(x)] \\ dx$ = $\\frac{1}{2}log(2\\pi\\sigma_2 ^2) + \\frac{1}{2\\sigma_2 ^2}[(\\mu_1 - \\mu_2)^2 + \\sigma_1 ^2] $ .....(1.3)<br>\n",
    "\n",
    "using eqn (1.2) and eqn (1.3) in eqn (1.1)<br>\n",
    "\n",
    "KL divergence = $- \\frac{(1 + log \\ 2\\pi\\sigma_1 ^2)}{2}$ + $\\frac{1}{2}log(2\\pi\\sigma_2 ^2) + \\frac{1}{2\\sigma_2 ^2}[(\\mu_1 - \\mu_2)^2 + \\sigma_1 ^2] $<br>\n",
    "\n",
    "\n",
    "$\\ \\ \\ \\ = log \\frac{\\sigma_2}{sigma_1} + \\frac{\\sigma_1 ^2 + (\\mu_1 - \\mu_2)^2}{2\\sigma_2 ^2} - \\frac{1}{2}$ <br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 2 <br><br>\n",
    "\n",
    "KL divergence = $\\int p(x) \\ log \\frac{p(x)}{q(x)} \\ dx$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL divergence for the data provided =  0.3181551004141449\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "df = pd.read_csv(\"data_KL.csv\")\n",
    "length = len(df)\n",
    "\n",
    "p_x = np.array(df.P)\n",
    "q_x = np.array(df.Q)\n",
    "\n",
    "KL_div = 0 \n",
    "for i in range(length): #i goes from 0 to 99\n",
    "    a = p_x[i]/q_x[i]\n",
    "\n",
    "    KL_div += p_x[i] * math.log(a)\n",
    "\n",
    "print(\"KL divergence for the data provided = \" , KL_div)\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
